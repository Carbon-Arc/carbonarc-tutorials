{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Data S3 Upload\n",
    "\n",
    "## Prerequisites\n",
    "- `carbonarc` library\n",
    "- `boto3` library\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. create .env file in the same directory as this notebook\n",
    "2. add the following lines to the .env file:\n",
    "```\n",
    "API_AUTH_TOKEN=<api auth token from (https://platform.carbonarc.co/profile)>\n",
    "AWS_ACCESS_KEY_ID=<aws access key id>\n",
    "AWS_SECRET_ACCESS_KEY=<aws secret access key>\n",
    "AWS_S3_BUCKET=<aws s3 bucket name>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required dependencies\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "import boto3\n",
    "from carbonarc import APIClient\n",
    "from carbonarc.manager import HttpRequestManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_AUTH_TOKEN=os.getenv(\"API_AUTH_TOKEN\")\n",
    "S3_BUCKET=os.getenv(\"S3_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_alldata_to_s3(\n",
    "    file_url: str, \n",
    "    request_manager: HttpRequestManager,\n",
    "    s3_bucket: str, \n",
    "    s3_key_prefix: str, \n",
    "    chunk_size: int = 5 * 1024 * 1024,  # Default to 100MB\n",
    "):\n",
    "    print(f\"Downloading file {file_url} to S3...\")\n",
    "    \n",
    "    # Ensure chunk size is at least 5MB (AWS requirement for multipart uploads)\n",
    "    if chunk_size < 5 * 1024 * 1024:\n",
    "        chunk_size = 5 * 1024 * 1024\n",
    "        print(\"Chunk size adjusted to 5MB to meet AWS minimum part size requirement\")\n",
    "    \n",
    "    # Make the request\n",
    "    response = request_manager.get_stream(file_url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Extract filename from response headers\n",
    "    filename = response.headers[\"Content-Disposition\"].split(\"filename=\")[1].strip('\"')\n",
    "    \n",
    "    # Create the full S3 key (path + filename)\n",
    "    s3_key = f\"{s3_key_prefix.rstrip('/')}/{filename}\"\n",
    "    \n",
    "    # Initialize S3 client with environment variables\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "        aws_session_token=os.environ.get('AWS_SESSION_TOKEN'),  # Optional, for temporary credentials\n",
    "        region_name=os.environ.get('AWS_REGION', 'us-east-1')  # Default to us-east-1 if not specified\n",
    "    )\n",
    "    \n",
    "    # Check if file is small enough for direct upload\n",
    "    content_length = int(response.headers.get('Content-Length', 0))\n",
    "    \n",
    "    # If file is small (less than 10MB) or content length is unknown, use simple upload\n",
    "    if content_length > 0 and content_length < 10 * 1024 * 1024:\n",
    "        print(f\"File is small ({content_length} bytes), using simple upload\")\n",
    "        content = response.content\n",
    "        s3_client.put_object(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=s3_key,\n",
    "            Body=content\n",
    "        )\n",
    "        print(f\"File uploaded successfully to s3://{s3_bucket}/{s3_key}\")\n",
    "        return f\"s3://{s3_bucket}/{s3_key}\"\n",
    "    \n",
    "    # For larger files, use multipart upload\n",
    "    print(f\"Initiating multipart upload to s3://{s3_bucket}/{s3_key}\")\n",
    "    multipart_upload = s3_client.create_multipart_upload(\n",
    "        Bucket=s3_bucket,\n",
    "        Key=s3_key\n",
    "    )\n",
    "    \n",
    "    upload_id = multipart_upload['UploadId']\n",
    "    parts = []\n",
    "    part_number = 1\n",
    "    \n",
    "    try:\n",
    "        # Use a buffer to collect chunks until we have at least 5MB\n",
    "        buffer = BytesIO()\n",
    "        buffer_size = 0\n",
    "        \n",
    "        for chunk in response.iter_content(chunk_size=1024 * 1024):  # Read in 1MB chunks\n",
    "            if not chunk:\n",
    "                continue\n",
    "                \n",
    "            # Add the chunk to our buffer\n",
    "            buffer.write(chunk)\n",
    "            buffer_size += len(chunk)\n",
    "            \n",
    "            # If we have at least 5MB (or this is the last chunk), upload the part\n",
    "            if buffer_size >= chunk_size:\n",
    "                # Reset buffer position to beginning for reading\n",
    "                buffer.seek(0)\n",
    "                \n",
    "                # Upload the part\n",
    "                part = s3_client.upload_part(\n",
    "                    Bucket=s3_bucket,\n",
    "                    Key=s3_key,\n",
    "                    PartNumber=part_number,\n",
    "                    UploadId=upload_id,\n",
    "                    Body=buffer.read()\n",
    "                )\n",
    "                \n",
    "                # Add the part info to our parts list\n",
    "                parts.append({\n",
    "                    'PartNumber': part_number,\n",
    "                    'ETag': part['ETag']\n",
    "                })\n",
    "                \n",
    "                print(f\"Uploaded part {part_number} ({buffer_size} bytes)\")\n",
    "                part_number += 1\n",
    "                \n",
    "                # Reset the buffer\n",
    "                buffer = BytesIO()\n",
    "                buffer_size = 0\n",
    "        \n",
    "        # Upload any remaining data as the final part (can be less than 5MB)\n",
    "        if buffer_size > 0:\n",
    "            buffer.seek(0)\n",
    "            part = s3_client.upload_part(\n",
    "                Bucket=s3_bucket,\n",
    "                Key=s3_key,\n",
    "                PartNumber=part_number,\n",
    "                UploadId=upload_id,\n",
    "                Body=buffer.read()\n",
    "            )\n",
    "            \n",
    "            parts.append({\n",
    "                'PartNumber': part_number,\n",
    "                'ETag': part['ETag']\n",
    "            })\n",
    "            \n",
    "            print(f\"Uploaded final part {part_number} ({buffer_size} bytes)\")\n",
    "        \n",
    "        # Complete the multipart upload only if we have parts\n",
    "        if parts:\n",
    "            s3_client.complete_multipart_upload(\n",
    "                Bucket=s3_bucket,\n",
    "                Key=s3_key,\n",
    "                UploadId=upload_id,\n",
    "                MultipartUpload={'Parts': parts}\n",
    "            )\n",
    "            \n",
    "            print(f\"File uploaded successfully to s3://{s3_bucket}/{s3_key}\")\n",
    "        else:\n",
    "            # No parts were uploaded, likely an empty file\n",
    "            s3_client.abort_multipart_upload(\n",
    "                Bucket=s3_bucket,\n",
    "                Key=s3_key,\n",
    "                UploadId=upload_id\n",
    "            )\n",
    "            \n",
    "            # Upload an empty file instead\n",
    "            s3_client.put_object(Bucket=s3_bucket, Key=s3_key, Body=b'')\n",
    "            print(f\"Empty file uploaded to s3://{s3_bucket}/{s3_key}\")\n",
    "            \n",
    "        return f\"s3://{s3_bucket}/{s3_key}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Abort the multipart upload if something goes wrong\n",
    "        s3_client.abort_multipart_upload(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=s3_key,\n",
    "            UploadId=upload_id\n",
    "        )\n",
    "        print(f\"Multipart upload aborted due to: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IDENTIFIERS = {\n",
    "    \"pos_instore_online_data\": {\n",
    "        \"outputdir\": \"output/pos/instore_online\",\n",
    "        \"filters\": {},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API Client\n",
    "assert API_AUTH_TOKEN, \"API_AUTH_TOKEN must be set in environment variables\"\n",
    "api_client = APIClient(API_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for each data identifier\n",
    "assert S3_BUCKET, \"S3_BUCKET must be set in environment variables\"\n",
    "for data_id, data in DATA_IDENTIFIERS.items():\n",
    "    # print(f\"Downloading data for {data_id}\")\n",
    "    params = data[\"filters\"]\n",
    "    outputdir = data[\"outputdir\"]\n",
    "     \n",
    "    # Get data manifest, this will contain all the files that can be downloaded\n",
    "    # You can track the downloaded files to maintain ingestion state\n",
    "    manifest = api_client.data.get_alldata_manifest(data_id)\n",
    "    print(f\"Data id: {data_id}, total files: {len(manifest['files'])}\")\n",
    "    # print(f\"Manifest: {manifest}\")\n",
    "    \n",
    "    # Download all files in the manifest, this can be done in parallel to speed up the process\n",
    "    for file in manifest[\"files\"]:\n",
    "        # Download the file to the output directory\n",
    "        print(f\"Downloading file {file}...\")\n",
    "        print(f\"{file['size_bytes']/1024/1024} MB\")\n",
    "        # Uncomment the line below to download files locally\n",
    "        # api_client.download_alldata_file(file[\"url\"], outputdir)\n",
    "        \n",
    "        # Download the file to S3\n",
    "        download_alldata_to_s3(\n",
    "            file[\"url\"],\n",
    "            api_client.data.request_manager,\n",
    "            s3_bucket=S3_BUCKET,\n",
    "            s3_key_prefix=outputdir,\n",
    "\n",
    "        )\n",
    "        \n",
    "    print(f\"Downloaded all files for {data_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
